{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo \n",
    "\n",
    "# Grupo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# usamos el dataset resultante del proceso de FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, balanced_accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = './'\n",
    "PATH_TO_TRAIN_FE_TEXT_LLM = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train_fe_text_llm.csv\")\n",
    "PATH_TO_MODELS = os.path.join(BASE_DIR, \"UA_MDM_LDI_II/work/models\")\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"UA_MDM_LDI_II/work/optuna_temp_artifacts\")\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"UA_MDM_LDI_II/work/optuna_artifacts\")\n",
    "\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos Tabulares\n",
    "dataset = pd.read_csv(PATH_TO_TRAIN_FE_TEXT_LLM)\n",
    "\n",
    "# Convertir columnas de objetos a categóricas o numéricas\n",
    "label_encoders = {}\n",
    "categorical_columns = ['SentimentLanguage', 'coherence', 'friendly_with_children', 'friendly_with_other_pets', 'is_calm']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    dataset[col] = le.fit_transform(dataset[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Split del dataset\n",
    "train, test = train_test_split(dataset, test_size=TEST_SIZE, random_state=SEED, stratify=dataset.AdoptionSpeed)\n",
    "\n",
    "# Seleccionar características y etiqueta\n",
    "features = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', \n",
    "            'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State', \n",
    "            'VideoAmt', 'PhotoAmt', 'IsPureBreed', 'AgeInYears', 'Healthy', 'IsFree', 'Breed1_Rarity', \n",
    "            'StatePopulation', 'IsPeninsular', 'SentimentScore', 'SentimentMagnitude', 'SentimentLanguage'] + \\\n",
    "           [col for col in dataset.columns if col.startswith('contains_')] + \\\n",
    "           ['coherence', 'requires_running_space', 'friendly_with_children', 'friendly_with_other_pets', \n",
    "            'is_calm', 'is_energetic']\n",
    "\n",
    "label = 'AdoptionSpeed'\n",
    "\n",
    "X_train = train[features]\n",
    "y_train = train[label]\n",
    "\n",
    "X_test = test[features]\n",
    "y_test = test[label]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento inicial\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(y_train.unique())\n",
    "}\n",
    "\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train, label=y_train)\n",
    "\n",
    "lgb_model = lgb.train(lgb_params, lgb_train_dataset)\n",
    "\n",
    "y_pred = lgb_model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "print(\"Cohen Kappa Score Inicial:\", cohen_kappa_score(y_test, y_pred, weights='quadratic'))\n",
    "display(plot_confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización de Hiperparámetros con Optuna\n",
    "def lgb_objective(trial):\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'verbosity': -1,\n",
    "        'num_class': len(y_train.unique()),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 300),  # Ampliado\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.3, 1.0),  # Ampliado\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.3, 1.0),  # Ampliado\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),  # Ampliado\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),  # Ampliado\n",
    "    }\n",
    "    \n",
    "    lgb_train_dataset = lgb.Dataset(data=X_train, label=y_train)\n",
    "    lgb_model = lgb.train(lgb_params, lgb_train_dataset)\n",
    "    \n",
    "    return cohen_kappa_score(y_test, lgb_model.predict(X_test).argmax(axis=1), weights='quadratic')\n",
    "\n",
    "# Función para Cross-Validation y Early Stopping\n",
    "def lgb_custom_metric_kappa(dy_pred, dy_true):\n",
    "    metric_name = 'kappa'\n",
    "    value = cohen_kappa_score(dy_true.get_label(), dy_pred.argmax(axis=1), weights='quadratic')\n",
    "    is_higher_better = True\n",
    "    return metric_name, value, is_higher_better\n",
    "\n",
    "def cv_es_lgb_objective(trial):\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'verbosity': -1,\n",
    "        'num_class': len(y_train.unique()),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 300),  # Ampliado\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.3, 1.0),  # Ampliado\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.3, 1.0),  # Ampliado\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),  # Ampliado\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),  # Ampliado\n",
    "    }\n",
    "\n",
    "    scores_ensemble = np.zeros((len(y_test), len(y_train.unique())))\n",
    "    score_folds = 0\n",
    "    n_splits = 5\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    for i, (if_index, oof_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        lgb_if_dataset = lgb.Dataset(data=X_train.iloc[if_index], label=y_train.iloc[if_index], free_raw_data=False)\n",
    "        lgb_oof_dataset = lgb.Dataset(data=X_train.iloc[oof_index], label=y_train.iloc[oof_index], free_raw_data=False)\n",
    "\n",
    "        lgb_model = lgb.train(\n",
    "            lgb_params,\n",
    "            lgb_if_dataset,\n",
    "            valid_sets=lgb_oof_dataset,\n",
    "            callbacks=[lgb.early_stopping(10, verbose=False)],\n",
    "            feval=lgb_custom_metric_kappa\n",
    "        )\n",
    "\n",
    "        scores_ensemble += lgb_model.predict(X_test)\n",
    "        score_folds += cohen_kappa_score(y_train.iloc[oof_index], lgb_model.predict(X_train.iloc[oof_index]).argmax(axis=1), weights='quadratic') / n_splits\n",
    "\n",
    "    predicted_filename = os.path.join(PATH_TO_TEMP_FILES, f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "    predicted_df = test.copy()\n",
    "    predicted_df['pred'] = [scores_ensemble[p, :] for p in range(scores_ensemble.shape[0])]\n",
    "    dump(predicted_df, predicted_filename)\n",
    "    upload_artifact(trial, predicted_filename, artifact_store)\n",
    "\n",
    "    cm_filename = os.path.join(PATH_TO_TEMP_FILES, f'cm_{trial.study.study_name}_{trial.number}.jpg')\n",
    "    plot_confusion_matrix(y_test, scores_ensemble.argmax(axis=1)).write_image(cm_filename)\n",
    "    upload_artifact(trial, cm_filename, artifact_store)\n",
    "\n",
    "    test_score = cohen_kappa_score(y_test, scores_ensemble.argmax(axis=1), weights='quadratic')\n",
    "    trial.set_user_attr(\"test_score\", test_score)\n",
    "\n",
    "    return score_folds\n",
    "\n",
    "# Crear estudio de Optuna\n",
    "study_name = \"05 - LGB Multiclass CV 03\"  # Nuevo nombre de estudio\n",
    "artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', storage=\"sqlite:///db.sqlite3\", study_name=study_name, load_if_exists=True)\n",
    "study.optimize(cv_es_lgb_objective, n_trials=300)  # Aumentar el número de trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los mejores hiperparámetros\n",
    "best_params = study.best_params\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "# Entrenar el modelo final con los mejores hiperparámetros\n",
    "lgb_params.update(best_params)\n",
    "lgb_train_dataset = lgb.Dataset(data=X_train, label=y_train)\n",
    "lgb_model = lgb.train(lgb_params, lgb_train_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred = lgb_model.predict(X_test).argmax(axis=1)\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Kappa: {kappa_score}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Balanced Accuracy: {balanced_accuracy}')\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "display(plot_confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Información adicional del modelo\n",
    "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred))\n",
    "lgb.plot_importance(lgb_model)\n",
    "plt.show()\n",
    "\n",
    "# Tamaño total del dataset de entrenamiento\n",
    "total_data = X_train.shape[0]\n",
    "train_data = X_train.shape[0]\n",
    "percentage_used = (train_data / total_data) * 100\n",
    "\n",
    "print(f\"Total de datos en el dataset de entrenamiento: {total_data}\")\n",
    "print(f\"Datos utilizados en el entrenamiento actual: {train_data}\")\n",
    "print(f\"Porcentaje de datos utilizados: {percentage_used:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
